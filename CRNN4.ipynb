{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOHKOrX4Kc7m3HN3wZQA7gh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kalwerake/Complement-Cross-Entropy/blob/master/CRNN4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CNN-LSTM 4**\n",
        "\n",
        "> layers remain the same. Slight changes in LSTM layer, fixed error in output reshaping. \n",
        "> using dfc with time window 22\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NWo0lOzg0yDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "x7HZ8_U5gw8J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f3f56ed-bafe-4fff-8377-bdcdc6f007d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pickle5"
      ],
      "metadata": {
        "id": "L5QJs0CAztRI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4d60465-a747-467c-8a3c-06f63e6d0b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.11.tar.gz (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.1/132.1 KB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: pickle5\n",
            "  Building wheel for pickle5 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pickle5: filename=pickle5-0.0.11-cp38-cp38-linux_x86_64.whl size=236308 sha256=3c5fddb9433565ce048ae7dabe910fff48ddaa6419b9197904f7b07373cc4230\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/d4/61/dbd8edd1a0d656be7b4267c85db3b61951eb60016a0154a122\n",
            "Successfully built pickle5\n",
            "Installing collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas=='1.5.2'"
      ],
      "metadata": {
        "id": "IE07WYaC3zUF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "496b8dd6-5258-4d07-ec9a-f5ed74dd5c06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pandas==1.5.2\n",
            "  Downloading pandas-1.5.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m88.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2) (1.21.6)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas==1.5.2) (2022.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.2) (1.15.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.3.5\n",
            "    Uninstalling pandas-1.3.5:\n",
            "      Successfully uninstalled pandas-1.3.5\n",
            "Successfully installed pandas-1.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwsfHpglABlr"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import pickle5 as pickle\n",
        "import sys\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "IYv_LkIR3tVe",
        "outputId": "b30c272e-8dde-4256-e195-f918605c0769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.5.2'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "main_dir = '/content/drive/MyDrive/Project'\n",
        "data_dir = '/content/drive/MyDrive/Project/dfc_cc200'\n",
        "pheno_dir = '/content/drive/MyDrive/Project/phenotype_files'"
      ],
      "metadata": {
        "id": "2yVs_neygs8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_p = '/content/drive/MyDrive/Project/phenotype_files/train_nn.csv'\n",
        "test_df_p = '/content/drive/MyDrive/Project/phenotype_files/test_nn.csv'"
      ],
      "metadata": {
        "id": "XPcjdBW-hK_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = pd.read_csv(train_df_p)\n",
        "test_df = pd.read_csv(test_df_p)"
      ],
      "metadata": {
        "id": "vjX7B4nqhaoU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DfcDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self,df,dfc_dir):\n",
        "        \"\"\"\n",
        "        :param df: pandas dataframe containing subject ids in column `FILE_ID` and target labels in column `TARGET`\n",
        "        :param dfc_dir: path to subdirectory containing the pickle files\n",
        "        \"\"\"\n",
        "        self.df_main = df\n",
        "        self.dfc_dir = dfc_dir\n",
        "        self.paths = [os.path.join(dfc_dir,f + '_dfc.pkl') for f in df.FILE_ID] # paths to pkl files\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        :param idx: index\n",
        "        :return: 4D tensor with 2 spatial dimensions and 1 temporal [N,1,90,200,200], and label\n",
        "        \"\"\"\n",
        "        dfc_pickle = pickle.load(file=open(self.paths[idx], \"rb\")) # load pkl file as dictionary\n",
        "\n",
        "        dfc_array = [val.to_numpy() for val in dfc_pickle.values()] # convert each dict value to a 2d list and make nested list of all values\n",
        "        dfc_tensor = [torch.from_numpy(npDfc).type(torch.float) for npDfc in dfc_array] #take each list in dfc_array and turn to torch tensor of type torch.float, otherwise result is float(64)\n",
        "\n",
        "        dfc_stack = torch.stack(dfc_tensor) # make tensor stack of each dfc array\n",
        "\n",
        "        label = self.df_main.loc[idx, 'TARGET'] # class label of idx as np array\n",
        "        label_tensor = torch.tensor(label)\n",
        "        label_tensor = label_tensor.to(torch.float32)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        return torch.unsqueeze(dfc_stack, dim= 0),torch.unsqueeze(label_tensor, dim = -1) # turn torch stack to 4D tensor for input into conv3d (channel,depth,height,width), return label as tensor of shape [batch size, 1]\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        :return: length of dataset\n",
        "        \"\"\"\n",
        "        return len(self.df_main)"
      ],
      "metadata": {
        "id": "ve8-kULfe3sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRNN4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CRNN4, self).__init__()\n",
        "\n",
        "        #Convolutional layers\n",
        "        self.seq_cnn = nn.Sequential(\n",
        "            nn.Conv3d(in_channels=1,out_channels=8, kernel_size=(2,200,1),stride=(1,1,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(in_channels=8,out_channels=16, kernel_size=(2,1,200),stride=(1,1,1)),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv3d(in_channels=16,out_channels=32, kernel_size=(8,1,1),stride=(2,1,1)),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "\n",
        "\n",
        "        #lstm layer\n",
        "        self.lstm1 = nn.LSTM(input_size=32, hidden_size=64, num_layers=1, batch_first = True)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.seq_dense = nn.Sequential(\n",
        "            nn.Linear(64,32),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(32,16),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(16,1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.seq_cnn(x)\n",
        "\n",
        "\n",
        "        x = torch.flatten(x,start_dim=2,end_dim=4) # reduce dimensionality for LSTM layer, to 3D tensor\n",
        "        x = x.permute(0,2,1) # transpose to make tensor of size [batch_size, sequence length, feature number]\n",
        "\n",
        "        x,_ = self.lstm1(x)\n",
        "\n",
        "        x = x[:,-1,:] # take output of last LSTM cell\n",
        "\n",
        "        x = self.seq_dense(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "NLY0MUIFe4xl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = DfcDataset(train_df, data_dir)\n",
        "train_dataloader = DataLoader(train_data, batch_size= 16,\n",
        "                                        shuffle=True,\n",
        "                                        num_workers=0)\n",
        "        \n",
        "test_data = DfcDataset(test_df, data_dir)\n",
        "test_dataloader = DataLoader(test_data, batch_size= 16,\n",
        "                                        shuffle=True,\n",
        "                                        num_workers=0)"
      ],
      "metadata": {
        "id": "pY1JgmHrfDh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrKJ8jLc9WAk",
        "outputId": "8acf867b-ebb5-4184-9276-50c8ceb727cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model4 = CRNN4()\n",
        "loss_fn = nn.BCELoss()\n",
        "optimiser =  optim.Adam(model4.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "FOESLTs76Pae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model4.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuTCOklB-h9Z",
        "outputId": "1282cd95-ceea-4777-858b-420bc8fba26e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CRNN4(\n",
              "  (seq_cnn): Sequential(\n",
              "    (0): Conv3d(1, 8, kernel_size=(2, 200, 1), stride=(1, 1, 1))\n",
              "    (1): ReLU()\n",
              "    (2): Conv3d(8, 16, kernel_size=(2, 1, 200), stride=(1, 1, 1))\n",
              "    (3): ReLU()\n",
              "    (4): Conv3d(16, 32, kernel_size=(8, 1, 1), stride=(2, 1, 1))\n",
              "    (5): ReLU()\n",
              "  )\n",
              "  (lstm1): LSTM(32, 64, batch_first=True)\n",
              "  (seq_dense): Sequential(\n",
              "    (0): Linear(in_features=64, out_features=32, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Dropout(p=0.25, inplace=False)\n",
              "    (3): Linear(in_features=32, out_features=16, bias=True)\n",
              "    (4): ReLU()\n",
              "    (5): Dropout(p=0.25, inplace=False)\n",
              "    (6): Linear(in_features=16, out_features=1, bias=True)\n",
              "    (7): Sigmoid()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss_history = []\n",
        "train_acc_history = []\n",
        "test_loss_history = []\n",
        "test_acc_history = []\n",
        "for i in range(200):\n",
        "  \n",
        "  running_loss = 0\n",
        "  epoch_accuracy = []\n",
        "  model4.train()\n",
        "  for j, (x_train,y) in enumerate(train_dataloader):\n",
        "    optimiser.zero_grad() # zero the gradient at each epoch start\n",
        "    y = y.to(device) # send y to cuda\n",
        "    x_train = x_train.to(device)\n",
        "    prediction = model4.forward(x_train)\n",
        "    loss = loss_fn(prediction,y) # loss\n",
        "\n",
        "    accuracy = (torch.round(prediction) == y).float().mean() # calculate accuracy for each mini-batch  take prediction tensor, reshape to 1d detach from computational graph turn to numpy array, round and see if rounded number is equal to label, find mean of this boolean array, this is the accuracy\n",
        "\n",
        "    running_loss += loss.item() # get epoch loss\n",
        "    epoch_accuracy.append(accuracy.item())\n",
        "                \n",
        "\n",
        "    loss.backward() # backward propgation\n",
        "    optimiser.step()\n",
        "  \n",
        "  mean_acc_train = np.mean(epoch_accuracy)\n",
        "  train_loss_history.append(running_loss)\n",
        "  train_acc_history.append(mean_acc_train)\n",
        "  \n",
        "  model4.eval()\n",
        "  test_loss_run = 0\n",
        "  test_acc_epoch = []\n",
        "  for j, (x_test,y_test) in enumerate(test_dataloader):\n",
        "    y_test = y_test.to(device)\n",
        "    x_test= x_test.to(device)\n",
        "    test_pred = model4.forward(x_test)\n",
        "    test_loss = loss_fn(test_pred,y_test) # loss\n",
        "\n",
        "    test_acc = (torch.round(test_pred) == y_test).float().mean() # calculate accuracy for each mini-batch  take prediction tensor, reshape to 1d detach from computational graph turn to numpy array, round and see if rounded number is equal to label, find mean of this boolean array, this is the accuracy\n",
        "\n",
        "    test_loss_run += test_loss.item()\n",
        "    test_acc_epoch.append(test_acc.item())\n",
        "\n",
        "  mean_acc_test = np.mean(test_acc_epoch)\n",
        "  test_acc_history.append(mean_acc_test)\n",
        "  test_loss_history.append(test_loss_run)\n",
        "  if (i+1)%5 == 0:\n",
        "    print(f'Epoch {i+1} train_loss: {round(running_loss,2)}, accuracy: {round(mean_acc_train,2)}, test_loss:{round(test_loss_run,2)}, test_acc: {round(mean_acc_test,2)} ')\n",
        "            \n",
        "\n"
      ],
      "metadata": {
        "id": "Dm4cDSbUm4RK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "outputId": "e7af52ce-8956-4863-f828-a41579c8ff68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 train_loss: 22.75, accuracy: 0.76, test_loss:8.96, test_acc: 0.59 \n",
            "Epoch 10 train_loss: 10.01, accuracy: 0.92, test_loss:11.45, test_acc: 0.68 \n",
            "Epoch 15 train_loss: 4.41, accuracy: 0.97, test_loss:20.4, test_acc: 0.61 \n",
            "Epoch 20 train_loss: 5.0, accuracy: 0.97, test_loss:25.58, test_acc: 0.59 \n",
            "Epoch 25 train_loss: 2.24, accuracy: 0.99, test_loss:21.69, test_acc: 0.61 \n",
            "Epoch 30 train_loss: 1.97, accuracy: 0.99, test_loss:24.95, test_acc: 0.61 \n",
            "Epoch 35 train_loss: 1.91, accuracy: 0.99, test_loss:25.73, test_acc: 0.63 \n",
            "Epoch 40 train_loss: 2.77, accuracy: 0.98, test_loss:21.18, test_acc: 0.64 \n",
            "Epoch 45 train_loss: 4.57, accuracy: 0.97, test_loss:19.49, test_acc: 0.6 \n",
            "Epoch 50 train_loss: 1.64, accuracy: 0.99, test_loss:24.91, test_acc: 0.64 \n",
            "Epoch 55 train_loss: 1.01, accuracy: 0.99, test_loss:28.21, test_acc: 0.63 \n",
            "Epoch 60 train_loss: 1.39, accuracy: 0.99, test_loss:31.67, test_acc: 0.63 \n",
            "Epoch 65 train_loss: 0.81, accuracy: 1.0, test_loss:36.6, test_acc: 0.62 \n",
            "Epoch 70 train_loss: 0.88, accuracy: 1.0, test_loss:42.61, test_acc: 0.63 \n",
            "Epoch 75 train_loss: 1.18, accuracy: 0.99, test_loss:27.47, test_acc: 0.63 \n",
            "Epoch 80 train_loss: 0.99, accuracy: 1.0, test_loss:33.64, test_acc: 0.63 \n",
            "Epoch 85 train_loss: 0.68, accuracy: 1.0, test_loss:41.06, test_acc: 0.61 \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-3c4a97075ef7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mepoch_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m   \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m   \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0moptimiser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# zero the gradient at each epoch start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# send y to cuda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    626\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 628\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    629\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 671\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-06043fed54f5>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0mD\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0mspatial\u001b[0m \u001b[0mdimensions\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0mtemporal\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m90\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mdfc_pickle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# load pkl file as dictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mdfc_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdfc_pickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# convert each dict value to a 2d list and make nested list of all values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36m_frombuffer\u001b[0;34m(buf, dtype, shape, order)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1854\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0m_frombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1855\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1856\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model4.state_dict(), '/content/drive/MyDrive/Project/model_params/CRNN4_params.pth') #save model parameters in google drive as pickle"
      ],
      "metadata": {
        "id": "h0rXFxxjgOIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "crnn2_train_metrics = {'train_acc_history': acc_history, 'test_loss_history': loss_history, 'test_acc': test_acc_history, 'test_loss': test_loss_history} # make dictionary of metrics\n",
        "\n",
        "mod_eval_p = '/content/drive/MyDrive/Project/model_evaluation'# path to subdirectory for model evaluation storage\n",
        "\n",
        "met_path = os.path.join(mod_eval_p, 'crnn4_train_metrics.pickle') # path to pickle name for metric storage\n",
        "\n",
        "with open(met_path, 'wb') as handle:\n",
        "    pickle.dump(crnn2_train_metrics, handle, protocol=pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "cnWcY_gVgYJV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "outputId": "d2532c23-af84-4157-b022-bc631d86c373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e2473c63a007>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcrnn2_train_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'train_acc_history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0macc_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_loss_history'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_acc'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_acc_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_loss_history\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m# make dictionary of metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmod_eval_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Project/model_evaluation'\u001b[0m\u001b[0;31m# path to subdirectory for model evaluation storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmet_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmod_eval_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'crnn4_train_metrics.pickle'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# path to pickle name for metric storage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'acc_history' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dDqzv8HqNY86"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}